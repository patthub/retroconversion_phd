{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYTHJ4Nqenw1"
      },
      "outputs": [],
      "source": [
        "# install any necessary packages\n",
        "!pip install -U spacy\n",
        "!pip install spacy_transformers\n",
        "\n",
        "# Install below packages if error occurs while installing above packages\n",
        "#!pip install -U pip setuptools\n",
        "# !pip install typing-extensions==4.6.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to the project folder\n",
        "%cd \"/content/drive/MyDrive/Doktorat/new_model\"\n",
        "\n",
        "# Import required libraries and install any necessary packages\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Check the installed version of spaCy\n",
        "spacy.__version__\n",
        "\n",
        "# Check GPU information\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "amHbgp9Ffolw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the annotated data from a JSON file\n",
        "cv_data = json.load(open('/content/drive/MyDrive/Doktorat/new_model/annotations_dataset/annotations_weird.json','r'))\n",
        "\n",
        "# Display the number of items in the dataset\n",
        "len(cv_data)\n",
        "\n",
        "# Display the first item in the dataset\n",
        "cv_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6ITGti-gutN",
        "outputId": "9110c7ba-86a9-4fe6-88b4-dbd126996b48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BIBLIOGRAFIA l* teoria literat ury OGÓLNE', {'entities': []}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/drive/MyDrive/Doktorat/new_model/config/base_config.cfg /content/drive/MyDrive/Doktorat/new_model/config/config.cfg"
      ],
      "metadata": {
        "id": "Tn6AviQIi1eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create spaCy DocBin objects from the annotated data\n",
        "def get_spacy_doc(file, data):\n",
        "  # Create a blank spaCy pipeline\n",
        "  nlp = spacy.blank('pl')\n",
        "  db = DocBin()\n",
        "\n",
        "  # Iterate through the data\n",
        "  for text, annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "\n",
        "    # Extract entities from the annotations\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start, end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start, end))\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if span is None:\n",
        "        # Log errors for annotations that couldn't be processed\n",
        "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
        "        file.write(err_data)\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return db"
      ],
      "metadata": {
        "id": "mE5bwInBjkVG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the annotated data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.2)\n",
        "\n",
        "# Display the number of items in the training and testing sets\n",
        "len(train), len(test)\n",
        "\n",
        "# Open a file to log errors during annotation processing\n",
        "file = open('/content/drive/MyDrive/Doktorat/new_model/trained_models/train_file.txt','w')\n",
        "\n",
        "# Create spaCy DocBin objects for training and testing data\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('/content/drive/MyDrive/Doktorat/new_model/trained_models/train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('/content/drive/MyDrive/Doktorat/new_model/trained_models/test_data.spacy')\n",
        "\n",
        "# Close the error log file\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH5GastgjqvR",
        "outputId": "7d526bf5-1d32-4c0d-9d60-b05bef4873ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 139/139 [00:00<00:00, 787.11it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 1108.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/drive/MyDrive/Doktorat/new_model/config/config.cfg  --output /content/drive/MyDrive/Doktorat/new_model/trained_models/output  --paths.train /content/drive/MyDrive/Doktorat/new_model/trained_models/train_data.spacy  --paths.dev /content/drive/MyDrive/Doktorat/new_model/trained_models/test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2vqmH75j5JQ",
        "outputId": "4e263007-97b0-4cb6-9e17-7acaa189ac20"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 18:56:18.035582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 18:56:18.035746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 18:56:18.037824: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 18:56:19.665010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/content/drive/MyDrive/Doktorat/new_model/trained_models/output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        8523.15    813.22    0.00    0.00    0.00    0.00\n",
            " 50     200      144414.50  59802.64   88.05   86.78   89.35    0.88\n",
            "100     400          49.33   7425.19   86.71   84.75   88.76    0.87\n",
            "150     600           1.66   7359.62   86.38   84.66   88.17    0.86\n",
            "200     800        6680.24  10333.49   88.70   86.93   90.53    0.89\n",
            "250    1000          37.61   7363.69   84.96   84.71   85.21    0.85\n",
            "300    1200         127.81   7331.29   85.88   85.38   86.39    0.86\n",
            "350    1400         128.69   7362.40   86.05   84.57   87.57    0.86\n",
            "400    1600         281.70   7342.19   85.12   85.63   84.62    0.85\n",
            "450    1800          32.41   7244.90   87.21   85.71   88.76    0.87\n",
            "500    2000           0.00   7176.89   84.88   83.43   86.39    0.85\n",
            "550    2200         425.53   7350.29   87.76   88.55   86.98    0.88\n",
            "600    2400          32.48   7092.38   87.65   87.13   88.17    0.88\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/content/drive/MyDrive/Doktorat/new_model/trained_models/output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/drive/MyDrive/Doktorat/new_model/trained_models/output/model-best')"
      ],
      "metadata": {
        "id": "SIzEB7NFsUbG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Szczęścia szukamy. Po 11:38-9\""
      ],
      "metadata": {
        "id": "-4k4sUV5sQcs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the extracted text using the loaded spaCy NER model\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through the named entities (entities) recognized by the model\n",
        "for ent in doc.ents:\n",
        "  # Print the recognized text and its corresponding label\n",
        "  print(ent.text, \"  ->>>>  \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4xeTIOsBiZ",
        "outputId": "cfc4c47b-5c5c-4146-c445-67efa15aa599"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Po   ->>>>   CZASOPISMO\n",
            "11:38-9   ->>>>   NUMER_CZASOPISMA\n"
          ]
        }
      ]
    }
  ]
}